{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746b9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c745e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7079\n",
      "2019\n",
      "1011\n"
     ]
    }
   ],
   "source": [
    "def get_nouns(phrase):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    mwe = nltk.tokenize.MWETokenizer([('dining', 'table'), ('cell', 'phone'),('wine','glass'),('parking','meter'),\n",
    "                                      ('hair','drier'),('fire','hydrant'),('traffic','light'),\n",
    "                                     ('baseball','glove'),('sports','ball'),('stop','sign'),\n",
    "                                     ('hot','dog')], separator=' ')\n",
    "\n",
    "    exceptions =['front','side','middle','top','part','edge']\n",
    "    include = ['oven']\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    #print(tokens)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokens) if is_noun(pos)]\n",
    "    #print(nouns)\n",
    "    aggregated = mwe.tokenize(nouns)#get the compound names\n",
    "    #print(aggregated)\n",
    "    filtered_nouns = []\n",
    "    for noun in aggregated:\n",
    "        tag = nltk.pos_tag([noun])\n",
    "        if (tag[0][1]=='NN' or tag[0][1]=='NNS' or tag[0][1]=='JJ') and noun not in exceptions or noun in include:\n",
    "            filtered_nouns.append(noun)\n",
    "    return tuple(filtered_nouns)\n",
    "\n",
    "def read_vsr_dataset(dataset_name, dataset_path = '../visual-spatial-reasoning/',splits_path='splits/', \n",
    "                         image_path = 'images/',sort = False, encode_labels = False):\n",
    "        dataset = pd.read_json(dataset_path+splits_path+dataset_name, lines =True)\n",
    "        dataset.rename(columns = {'caption':'hypothesis', 'image':'Flickr30kID', 'label' : 'gold_label'}, inplace = True)\n",
    "        dataset['Flickr30kID']=dataset['Flickr30kID'].apply(lambda img_name: dataset_path + image_path + img_name )\n",
    "        if encode_labels:\n",
    "            labels_encoding = {0:0,1:2}#leave the label 0 the same and convert 1 to 2 to mean entailment\n",
    "            dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        if(dataset_name=='train.json'):\n",
    "            dataset.drop(labels=[1786,3569,4553,4912], axis=0, inplace = True)\n",
    "        elif(dataset_name=='test.json'):\n",
    "            dataset.drop(labels=[135,614,1071,1621,1850], axis=0, inplace = True)\n",
    "        elif(dataset_name=='dev.json'):\n",
    "            dataset.drop(labels=[807], axis=0, inplace = True)\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        if sort:\n",
    "            dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "        return dataset\n",
    "    \n",
    "train = read_vsr_dataset('train.json')\n",
    "train['entities'] = train['hypothesis'].apply(lambda x: get_nouns(x))\n",
    "test = read_vsr_dataset('test.json')\n",
    "dev = read_vsr_dataset('dev.json')\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "print(len(dev.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40552c",
   "metadata": {},
   "source": [
    "# Rule formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d2832b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "#Get a set of the captions\n",
    "captions_set = set()\n",
    "for index, row in train.iterrows():\n",
    "        caption = row['hypothesis']\n",
    "        captions_set.add(caption)\n",
    "        \n",
    "same_cap = {}\n",
    "for caption in list(captions_set):\n",
    "    same_cap_df = train[(train.hypothesis==caption) & (train.gold_label==1)].reset_index(drop=True)\n",
    "    if(len(same_cap_df.index)>2):\n",
    "        same_cap[caption]= same_cap_df\n",
    "print(len(same_cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2d25cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873\n"
     ]
    }
   ],
   "source": [
    "entities_set = dict()\n",
    "for index, row in train.iterrows():\n",
    "        entities = tuple(row['entities'])\n",
    "        if(entities not in entities_set):\n",
    "            entities_set[entities]=0\n",
    "        entities_set[entities]+=1\n",
    "        \n",
    "same_ent = {}\n",
    "for ent in list(entities_set.keys()):\n",
    "    same_ent_df = train[(train.entities==ent)].reset_index()\n",
    "    if(len(same_ent_df.index)>=2):\n",
    "        same_ent[ent]=same_ent_df \n",
    "print(len(same_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62a5f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = {1:['at the right side of'],\n",
    "           0:['at the left side of']}\n",
    "group2 = {1:['at the edge of'],\n",
    "           0:['at the back of']}\n",
    "group3= {1:['facing'],\n",
    "           0:['facing away from']}\n",
    "group4 = {1:['parallel to'],\n",
    "           0:['perpendicular to']}\n",
    "group5 = {1:['connected to'],\n",
    "           0:['detached from']}\n",
    "group6= {1:['has as a part'],\n",
    "           0:['part of']}\n",
    "group7 = {1:['inside'],\n",
    "           0:['outside']}\n",
    "group8 = {1:['inside'],\n",
    "           0:['out of']}\n",
    "group9 = {1:['in'],\n",
    "           0:['out of']}\n",
    "group10 = {1:['in'],\n",
    "           0:['outside']}\n",
    "group11 = {1:['within'],\n",
    "           0:['outside']}\n",
    "group12 = {1:['within'],\n",
    "           0:['out of']}\n",
    "group13 = {1:['close to'],\n",
    "           0:['far from']}\n",
    "group14 = {1:['close to'],\n",
    "           0:['far away from']}\n",
    "group15 = {1:['near'],\n",
    "           0:['far from']}\n",
    "group16 = {1:['near'],\n",
    "           0:['far away from']}\n",
    "group17 = {1:['on top of'],\n",
    "           0:['beneath']}\n",
    "group18 = {1:['left of'],\n",
    "           0:['right of']}\n",
    "group19 = {1:['on top of'],\n",
    "           0:['under']}\n",
    "group20 = {1:['on top of'],\n",
    "           0:['below']}\n",
    "group21 = {1:['above'],\n",
    "           0:['below']}\n",
    "group22 = {1:['over'],\n",
    "           0:['under']}\n",
    "group23 = {1:['in the middle of'],\n",
    "           0:['at the edge of']}\n",
    "group24 = {1:['away from'],\n",
    "           0:['close to']}\n",
    "group25 = {1:['away from'],\n",
    "           0:['toward']}\n",
    "group26 = {1:['next to'],\n",
    "           0:['far from']}\n",
    "group27 = {1:['next to'],\n",
    "           0:['far away from']}\n",
    "group28 = {1:['facing'],\n",
    "           0:['opposite to']}\n",
    "\n",
    "groups = [group1, group2, group3, group4, group5, group6, group7, group8, group9, group10, group11,\n",
    "            group12, group13, group14, group15, group16, group17, group18, group19, group20, group21,\n",
    "                group22, group23, group24, group25, group26, group27, group28]\n",
    "\n",
    "#groups of relations where the order of the entities in the phrase matters to determine the spatial relation\n",
    "asymetric_groups = [group1,group2,group6,group7,group8,group9,group10,group11,group12,group17,\n",
    "                    group18,group19,group20,group21,group22,group23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "832d85ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "rule1 = {}\n",
    "for caption in list(captions_set):\n",
    "    label = 0\n",
    "    df_zero = train[(train.hypothesis==caption) & (train.gold_label==label)].reset_index(drop=True)\n",
    "    if(len(df_zero.index)>=2):\n",
    "        rule1[tuple([caption,label])]= df_zero\n",
    "    label = 1\n",
    "    df_one = train[(train.hypothesis==caption) & (train.gold_label==label)].reset_index(drop=True)\n",
    "    if(len(df_one.index)>=2):\n",
    "        rule1[tuple([caption,label])]= df_one\n",
    "rule1_data = list(rule1.values())\n",
    "print(len(rule1))\n",
    "#show_dataset(rule1_data[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d700b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "rule2 = []\n",
    "count = 0\n",
    "for ent in same_ent:\n",
    "    ent_df = same_ent[ent]\n",
    "    count+=1\n",
    "    if(count==10):\n",
    "        break\n",
    "    for group in groups:\n",
    "        negative_relation = group[0][0]\n",
    "        positive_relation = group[1][0]\n",
    "        label_positives = 0\n",
    "        label_negatives = 1\n",
    "        positives_df = ent_df[(ent_df.relation == positive_relation) & (ent_df.gold_label==label_positives)].reset_index(drop=True)\n",
    "        negatives_df = ent_df[(ent_df.relation == negative_relation) & (ent_df.gold_label==label_negatives)].reset_index(drop=True)\n",
    "        first_df = pd.concat([positives_df,negatives_df]).reset_index()\n",
    "        if(len(first_df.index)>1):\n",
    "            rule2.append(first_df)\n",
    "        label_positives = 1\n",
    "        label_negatives = 0\n",
    "        positives_df = ent_df[(ent_df.relation == positive_relation) & (ent_df.gold_label==label_positives)].reset_index(drop=True)\n",
    "        negatives_df = ent_df[(ent_df.relation == negative_relation) & (ent_df.gold_label==label_negatives)].reset_index(drop=True)\n",
    "        second_df = pd.concat([positives_df,negatives_df]).reset_index(drop=True)\n",
    "        if(len(second_df.index)>1):\n",
    "            rule2.append(second_df)\n",
    "print(len(rule2))\n",
    "#show_dataset(rule2[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff593cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
