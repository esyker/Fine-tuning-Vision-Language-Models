{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a6a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "from transformers import CLIPTokenizer\n",
    "from transformers import CLIPFeatureExtractor\n",
    "from transformers import CLIPProcessor\n",
    "from transformers import CLIPModel, CLIPConfig\n",
    "from transformers import get_scheduler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56040867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset_path, image_path,max_length = 90):\n",
    "        self.max_length = max_length\n",
    "        self.dataset = self.read_dataset(dataset_path)\n",
    "        self.image_path = image_path\n",
    "        #self.features_extract = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        #self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def read_dataset(self, url,sort = False):\n",
    "        dataset = pd.read_csv(url)\n",
    "        labels_encoding = {'contradiction':0,'neutral': 1,\n",
    "                           'entailment':2}\n",
    "        dataset = dataset[['hypothesis','Flickr30kID','gold_label']]\n",
    "        dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        if sort:\n",
    "            dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "        return dataset\n",
    "    \n",
    "    def get_visual_features(self,img):\n",
    "        return self.features_extract(img)\n",
    "    \n",
    "    def get_text_features(self,text): \n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataset.loc[idx,'Flickr30kID']\n",
    "        text = self.dataset.loc[idx,'hypothesis']\n",
    "        label = self.dataset.loc[idx,'gold_label']\n",
    "        item = self.processor(text=text, images=Image.open(self.image_path+img_name), return_tensors=\"pt\",padding=\"max_length\", max_length=68,truncation=True)\n",
    "        item['input_ids'] = item['input_ids'][0]\n",
    "        item['attention_mask'] = item['attention_mask'][0]\n",
    "        item['pixel_values'] = item['pixel_values'][0]\n",
    "        item['label'] = torch.tensor(label)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.index)\n",
    "    \n",
    "    def __exit__(self):\n",
    "        self.img_env.close()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becbbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer():\n",
    "    def __init__(self,model,train,eval_test, device = None, num_labels = 3):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.eval_test = eval_test\n",
    "        self.test_acc_list = []#init\n",
    "        self.model_path = \"./models/my_model_epoch_\"\n",
    "        self.num_labels = num_labels\n",
    "        self.config_problem_type = \"single_label_classification\"\n",
    "        if self.config_problem_type == \"single_label_classification\":\n",
    "          self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "          self.output_loss = lambda output,labels : self.loss_fct(output.logits.view(-1, self.num_labels), labels.view(-1)) \n",
    "        elif self.config_problem_type == \"regression\":\n",
    "          self.loss_fct = torch.nn.MSELoss()\n",
    "          if self.num_labels == 1: self.output_loss = lambda output,labels : self.loss_fct(output.logits.squeeze(), labels.squeeze())\n",
    "          else: self.output_loss =  lambda output,labels : self.loss_fct(output.logits, labels)\n",
    "        elif self.config_problem_type == \"multi_label_classification\":\n",
    "          self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "          self.output_loss = lambda output,labels : self.loss_fct(output.logits, labels)\n",
    "\n",
    "    def train_model(self,batch_size = None, lr= None, epochs=None):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(self.train, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps= epochs * len(train_loader)\n",
    "        )\n",
    "        for epoch in range(epochs):\n",
    "            progress_bar = tqdm(range(math.ceil(len(self.train)/batch_size)))\n",
    "            train_losses = []\n",
    "            for item in train_loader:\n",
    "                item['input_ids'] = item['input_ids'].to(self.device)\n",
    "                item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "                item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "                item['label'] = item['label'].to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model.forward(**item)\n",
    "                label = item['label']\n",
    "                loss = self.output_loss(outputs, label)\n",
    "                train_losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                progress_bar.update(1)\n",
    "            print(\"Saving model ....\")\n",
    "            model.save_model(self.model_path+str(epoch))\n",
    "            print(\"Model Saved!\")\n",
    "            test_acc = self.eval_test.evaluate(batch_size = batch_size)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            print('--- Epoch ',epoch,' Acc: ',test_acc)\n",
    "            mean_loss = torch.tensor(train_losses).mean().item()\n",
    "            print('Training loss: %.4f' % (mean_loss))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc345869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEvaluator():\n",
    "  def __init__(self,model,test, device = None):\n",
    "    self.test_dataset = test\n",
    "    self.model = model\n",
    "    self.device =device\n",
    "    \n",
    "  def evaluate(self, batch_size = 64):\n",
    "      self.model.eval()\n",
    "      loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle = False, num_workers = 4)\n",
    "      n_correct = 0\n",
    "      n_possible = 0\n",
    "      for item in loader:\n",
    "        item['input_ids'] = item['input_ids'].to(self.device)\n",
    "        item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "        item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "        item['label'] = item['label'].to(self.device)\n",
    "        y_hat = self.model.predict(item)\n",
    "        y = item['label']\n",
    "        n_correct += (y == y_hat).sum().item()\n",
    "        n_possible += float(y.shape[0])\n",
    "      self.model.train()\n",
    "      return n_correct / n_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be003c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(CLIPModel):\n",
    "    def __init__(self, num_labels=3):\n",
    "      #super().__init__(CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\"))\n",
    "      super().__init__(CLIPConfig.from_pretrained(\"flax-community/clip-rsicd-v2\"))\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      self.num_labels = num_labels\n",
    "      \n",
    "      self.config.problem_type = \"single_label_classification\"\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = super().forward(input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions,output_hidden_states, return_dict=True)\n",
    "\n",
    "        aux_vision = output.vision_model_output[0]\n",
    "        aux_vision = self.visual_projection(aux_vision) \n",
    "        aux_text = output.text_model_output[0]\n",
    "        aux_text = self.text_projection(aux_text)\n",
    "\n",
    "        aux = torch.cat((aux_vision, aux_text), dim=1)\n",
    "\n",
    "        aux = aux.reshape((aux.size()[1], aux.size()[0], aux.size()[2]))\n",
    "\n",
    "        vision_mask = torch.ones((aux_vision.size()[0], aux_vision.size()[1])).to(device)\n",
    "\n",
    "        multi_modal_mask = torch.cat((vision_mask, attention_mask), dim=1).to(device)\n",
    "\n",
    "\n",
    "        aux = self.new_transformer_encoder(aux, src_key_padding_mask=multi_modal_mask) \n",
    "        aux = aux.reshape((aux.size()[1], aux.size()[0], aux.size()[2])) #change back shape to (batch size, sequence length, features)\n",
    "\n",
    "        multi_modal_mask = multi_modal_mask.unsqueeze(2).expand(-1,-1, aux.size()[2])\n",
    "\n",
    "        #TODO experimentar a mask inicial e ver se ele faz as contas bem na mesma, se nao deixar ficar assim \n",
    "        aux = torch.sum(aux * multi_modal_mask, 1) / torch.clamp(multi_modal_mask.sum(1), min=1e-9)\n",
    "        aux = self.classification(aux)\n",
    "\n",
    "        output.logits = aux\n",
    "        return output\n",
    "    \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2891937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(CLIPModel):\n",
    "    def __init__(self, num_labels=3):\n",
    "      #super().__init__(CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\"))\n",
    "      super().__init__(CLIPConfig.from_pretrained(\"flax-community/clip-rsicd-v2\"))\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      self.num_labels = num_labels\n",
    "      \n",
    "      self.config.problem_type = \"single_label_classification\"\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = super().forward(input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict=True)\n",
    "        \n",
    "        print(output.vision_model_output[0].shape)\n",
    "        print(output.text_model_output[0].shape)\n",
    "        \n",
    "        aux_vision = output.vision_model_output[0]\n",
    "        aux_vision = self.visual_projection(aux_vision) \n",
    "        \n",
    "        aux_text = output.text_model_output[0]\n",
    "        aux_text = self.text_projection(aux_text)\n",
    "        \n",
    "        print(aux_text.shape)\n",
    "        print(aux_vision.shape)\n",
    "        \n",
    "        aux = torch.cat((aux_vision, aux_text), dim=1)\n",
    "        \n",
    "        print(aux.shape)\n",
    "        \n",
    "        aux = aux.reshape((aux.size()[1], aux.size()[0], aux.size()[2]))\n",
    "        \n",
    "        print(aux.shape)\n",
    "    \n",
    "        vision_mask = torch.ones((aux_vision.size()[0], aux_vision.size()[1])).to(device)\n",
    "        \n",
    "        print(vision_mask.shape)\n",
    "\n",
    "        multi_modal_mask = torch.cat((vision_mask, attention_mask), dim=1).to(device)\n",
    "        \n",
    "        print(multi_modal_mask.shape)\n",
    "\n",
    "        aux = self.new_transformer_encoder(aux, src_key_padding_mask=multi_modal_mask)\n",
    "        aux = aux.reshape((aux.size()[1], aux.size()[0], aux.size()[2])) #change back shape to (batch size, sequence length, features)\n",
    "        \n",
    "        print(aux.shape)\n",
    "        clamp = torch.clamp(multi_modal_mask.sum(1), min=1e-9)\n",
    "        print(multi_modal_mask.unsqueeze(-1).shape)\n",
    "        multiplication = aux * multi_modal_mask\n",
    "        aux = torch.sum(multiplication, 1) / clamp\n",
    "\n",
    "        output.logits = self.classification(aux)\n",
    "    \n",
    "        return output\n",
    "  \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82aff68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(CLIPModel):\n",
    "    def __init__(self, num_labels=3):\n",
    "      super().__init__(CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\"))\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = super().forward(input_ids,  pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict=True)\n",
    "\n",
    "        aux_vision = output.vision_model_output[0]\n",
    "        aux_vision = self.visual_projection(aux_vision)\n",
    "        aux_text = output.text_model_output[0]\n",
    "        aux_text = self.text_projection(aux_text)\n",
    "        aux = torch.cat((aux_vision,aux_text),dim=1)\n",
    "\n",
    "        ones = torch.ones(aux_vision.shape[0],aux_vision.shape[1],dtype=torch.float).to(device)\n",
    "        aux_mask = torch.cat((ones,attention_mask), dim=1)\n",
    "        padding_mask = torch.swapaxes(aux_mask, 0, 1)\n",
    "\n",
    "        aux = self.new_transformer_encoder( aux, src_key_padding_mask= padding_mask)\n",
    "        \n",
    "        input_mask_expanded = aux_mask.unsqueeze(-1).expand(aux.size()).float()\n",
    "        \n",
    "        aux = torch.sum(aux * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        output.logits = self.classification(aux)\n",
    "        return output\n",
    "    \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee62d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfs/home/u121170/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:83: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1343c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MyDataset('../e-ViL/data/esnlive_train.csv','../e-ViL/data/flickr30k_images/flickr30k_images/', max_length=90)\n",
    "test = MyDataset('../e-ViL/data/esnlive_test.csv','../e-ViL/data/flickr30k_images/flickr30k_images/', max_length=90)\n",
    "dev = MyDataset('../e-ViL/data/esnlive_dev.csv','../e-ViL/data/flickr30k_images/flickr30k_images/', max_length=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9effaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIP()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c54dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training Model-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 51/6277 [02:14<4:28:44,  2.59s/it]"
     ]
    }
   ],
   "source": [
    "task = 'train'\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "if task =='train':\n",
    "    test_evaluator = MyEvaluator(model,test)\n",
    "    dev_evaluator = MyEvaluator(model,dev)\n",
    "    trainer = MyTrainer(model,train,test_evaluator)\n",
    "    print(\"-----Training Model-----\")\n",
    "    trainer.train_model(epochs=epochs ,batch_size = batch_size, lr = lr)\n",
    "    print('----Training finished-----')\n",
    "    dev_acc = dev_evaluator.evaluate(batch_size = batch_size)\n",
    "    print(\"---- Dev Acc: \",dev_acc)\n",
    "elif task =='test':\n",
    "    model.load_model(\"/content/drive/MyDrive/teses/tese_MECD/implementation/my_model\")\n",
    "    evaluator = MyEvaluator(model,dev)\n",
    "    acc = evaluator.evaluate(batch_size = batch_size)\n",
    "    print(acc)\n",
    "    #output = run_example(model,train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
