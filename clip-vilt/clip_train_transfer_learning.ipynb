{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a6a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "from transformers import CLIPTokenizer\n",
    "from transformers import CLIPFeatureExtractor\n",
    "from transformers import CLIPProcessor\n",
    "from transformers import CLIPModel, CLIPConfig\n",
    "from transformers import get_scheduler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56040867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset_path, image_path,only_vsr = False, vsr = None, max_length = 90):\n",
    "        self.max_length = max_length\n",
    "        self.image_path = image_path\n",
    "        if(only_vsr):#assess only for the vsr dataset\n",
    "            self.dataset = self.read_vsr_dataset(vsr)\n",
    "        else:#assess for the SNLI-VE dataset\n",
    "            self.dataset = self.read_dataset(dataset_path)\n",
    "            if(vsr):#add extra data to make model more reliable\n",
    "                self.dataset = pd.concat([self.dataset, self.read_vsr_dataset('train.json'),\n",
    "                                self.read_vsr_dataset('test.json'), self.read_vsr_dataset('dev.json')], ignore_index=True)\n",
    "        #self.features_extract = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        #self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def read_vsr_dataset(self,dataset_name, dataset_path = '../visual-spatial-reasoning/',splits_path='splits/', \n",
    "                         image_path = 'images/',sort = False):\n",
    "        dataset = pd.read_json(dataset_path+splits_path+dataset_name, lines =True)\n",
    "        labels_encoding = {0:0,1:2}#leave the label 0 the same and convert 1 to 2 to mean entailment\n",
    "        dataset = dataset[['caption','image','label']]\n",
    "        dataset.rename(columns = {'caption':'hypothesis', 'image':'Flickr30kID', 'label' : 'gold_label'}, inplace = True)\n",
    "        dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        dataset['Flickr30kID']=dataset['Flickr30kID'].apply(lambda path: dataset_path+image_path+path)\n",
    "        if(dataset_name=='train.json'):\n",
    "            dataset.drop(labels=[1786,3569,4553,4912], axis=0, inplace = True)\n",
    "        elif(dataset_name=='test.json'):\n",
    "            dataset.drop(labels=[135,614,1071,1621,1850], axis=0, inplace = True)\n",
    "        elif(dataset_name=='dev.json'):\n",
    "            dataset.drop(labels=[807], axis=0, inplace = True)\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        if sort:\n",
    "            dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "        return dataset\n",
    "    \n",
    "    def read_dataset(self, url,sort = False):\n",
    "        dataset = pd.read_csv(url)\n",
    "        labels_encoding = {'contradiction':0,'neutral': 1,\n",
    "                           'entailment':2}\n",
    "        dataset = dataset[['hypothesis','Flickr30kID','gold_label']]\n",
    "        dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        dataset['Flickr30kID']=dataset['Flickr30kID'].apply(lambda path: self.image_path+path)\n",
    "        if sort:\n",
    "            dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "        return dataset\n",
    "    \n",
    "    def get_visual_features(self,img):\n",
    "        return self.features_extract(img)\n",
    "    \n",
    "    def get_text_features(self,text): \n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataset.loc[idx,'Flickr30kID']\n",
    "        text = self.dataset.loc[idx,'hypothesis']\n",
    "        label = self.dataset.loc[idx,'gold_label']\n",
    "        item = self.processor(text=text, images=Image.open(img_name), return_tensors=\"pt\",padding=\"max_length\", \n",
    "                              max_length= self.max_length,truncation=True)\n",
    "        item['input_ids'] = item['input_ids'][0]\n",
    "        item['attention_mask'] = item['attention_mask'][0]\n",
    "        item['pixel_values'] = item['pixel_values'][0]\n",
    "        item['label'] = torch.tensor(label)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.index)\n",
    "    \n",
    "    def __exit__(self):\n",
    "        self.img_env.close()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becbbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer():\n",
    "    def __init__(self,model,train,eval_test, device = None, num_labels = 3):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.eval_test = eval_test\n",
    "        self.test_acc_list = []#init\n",
    "        self.model_path = \"./models/my_model_epoch_\"\n",
    "        self.num_labels = num_labels\n",
    "        self.config_problem_type = \"single_label_classification\"\n",
    "        if self.config_problem_type == \"single_label_classification\":\n",
    "          self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "          self.output_loss = lambda output,labels : self.loss_fct(output.logits.view(-1, self.num_labels), labels.view(-1)) \n",
    "        elif self.config_problem_type == \"regression\":\n",
    "          self.loss_fct = torch.nn.MSELoss()\n",
    "          if self.num_labels == 1: self.output_loss = lambda output,labels : self.loss_fct(output.logits.squeeze(), labels.squeeze())\n",
    "          else: self.output_loss =  lambda output,labels : self.loss_fct(output.logits, labels)\n",
    "        elif self.config_problem_type == \"multi_label_classification\":\n",
    "          self.loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "          self.output_loss = lambda output,labels : self.loss_fct(output.logits, labels)\n",
    "\n",
    "    def train_model(self,batch_size = None, lr= None, epochs=None):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        train_loader = DataLoader(self.train, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps= epochs * len(train_loader)\n",
    "        )\n",
    "        for epoch in range(epochs):\n",
    "            progress_bar = tqdm(range(math.ceil(len(self.train)/batch_size)))\n",
    "            train_losses = []\n",
    "            for item in train_loader:\n",
    "                item['input_ids'] = item['input_ids'].to(self.device)\n",
    "                item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "                item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "                item['label'] = item['label'].to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model.forward(**item)\n",
    "                label = item['label']\n",
    "                loss = self.output_loss(outputs, label)\n",
    "                train_losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                progress_bar.update(1)\n",
    "            #print(\"Saving model ....\")\n",
    "            #model.save_model(self.model_path+str(epoch))\n",
    "            #print(\"Model Saved!\")\n",
    "            test_acc = self.eval_test.evaluate(batch_size = batch_size)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            print('--- Epoch ',epoch,' Acc: ',test_acc)\n",
    "            mean_loss = torch.tensor(train_losses).mean().item()\n",
    "            print('Training loss: %.4f' % (mean_loss))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc345869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEvaluator():\n",
    "  def __init__(self,model,test, device = None):\n",
    "    self.test_dataset = test\n",
    "    self.model = model\n",
    "    self.device = device\n",
    "  \n",
    "  def evaluate(self, batch_size = 64):\n",
    "      self.model.eval()\n",
    "      loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle = False)\n",
    "      n_correct = 0\n",
    "      n_possible = 0\n",
    "      for item in loader:\n",
    "        item['input_ids'] = item['input_ids'].to(self.device)\n",
    "        item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "        item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "        item['label'] = item['label'].to(self.device)\n",
    "        y_hat = self.model.predict(item)\n",
    "        y = item['label']\n",
    "        n_correct += (y == y_hat).sum().item()\n",
    "        n_possible += float(y.shape[0])\n",
    "      self.model.train()\n",
    "      return n_correct / n_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82aff68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(CLIPModel):\n",
    "    def __init__(self, num_labels=3):\n",
    "      super().__init__(CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\"))\n",
    "      #super().__init__(CLIPConfig.from_pretrained(\"flax-community/clip-rsicd-v2\"))\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = super().forward(input_ids,  pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict=True)\n",
    "        #print('ov',output.vision_model_output[0].size())\n",
    "        #print('ot',output.text_model_output[0].size())\n",
    "        #print('am',attention_mask.size())\n",
    "        aux_vision = output.vision_model_output[0]#.pooler_output#\n",
    "        aux_vision = self.visual_projection(aux_vision)\n",
    "        aux_text = output.text_model_output[0]#.pooler_output#[0]\n",
    "        aux_text = self.text_projection(aux_text)\n",
    "        aux = torch.cat((aux_vision,aux_text),dim=1)\n",
    "\n",
    "        ones = torch.ones(aux_vision.shape[0],aux_vision.shape[1],dtype=torch.float).to(device)\n",
    "        aux_mask = torch.cat((ones,attention_mask), dim=1)\n",
    "        padding_mask = torch.swapaxes(aux_mask, 0, 1)\n",
    "\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask',aux_mask.size())\n",
    "\n",
    "        aux = self.new_transformer_encoder( aux, src_key_padding_mask= padding_mask)\n",
    "        #aux = self.new_transformer_encoder( aux, mask= padding_mask)\n",
    "        #print(aux.shape)#change back shape to (batch size, sequence length, features)\n",
    "        \n",
    "        input_mask_expanded = aux_mask.unsqueeze(-1).expand(aux.size()).float()\n",
    "        #print('input_mask_expanded ',input_mask_expanded.size())\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask ',aux_mask.size())\n",
    "        #aux = torch.sum(aux * aux_mask, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        aux = torch.sum(aux * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        output.logits = self.classification(aux)\n",
    "        return output\n",
    "    \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee62d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "train = MyDataset('../e-ViL/data/esnlive_train.csv','../e-ViL/data/flickr30k_images/flickr30k_images/',\n",
    "                  max_length= max_length, vsr = 'train.json', only_vsr = True)\n",
    "test = MyDataset('../e-ViL/data/esnlive_test.csv','../e-ViL/data/flickr30k_images/flickr30k_images/',max_length=max_length\n",
    "                            , vsr = 'test.json', only_vsr = True)\n",
    "dev = MyDataset('../e-ViL/data/esnlive_dev.csv','../e-ViL/data/flickr30k_images/flickr30k_images/',max_length=max_length\n",
    "                           , vsr = 'dev.json', only_vsr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc643e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9effaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abde259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (new_encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (new_transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classification): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#modules = [model.text_model.embeddings, model.vision_model.embeddings]\n",
    "modules = [model.text_model, model.vision_model]\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c54dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'train'\n",
    "model_path = 'vsr_len32_batch32_lr1e-5'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 1e-5\n",
    "if task =='train':\n",
    "    test_evaluator = MyEvaluator(model,test, device = device)\n",
    "    dev_evaluator = MyEvaluator(model,dev, device = device)\n",
    "    trainer = MyTrainer(model,train,test_evaluator, device = device)\n",
    "    model = model.to(device)\n",
    "    print(\"-----Training Model-----\")\n",
    "    trainer.train_model(epochs=epochs ,batch_size = batch_size, lr = lr)\n",
    "    print('----Training finished-----')\n",
    "    dev_acc = dev_evaluator.evaluate(batch_size = batch_size)\n",
    "    print(\"---- Dev Acc: \",dev_acc)\n",
    "    model.save_model(model_path)\n",
    "elif task =='test':\n",
    "    model.load_model(\"./models/my_model_epoch_9\")\n",
    "    model = model.to(device)\n",
    "    evaluator = MyEvaluator(model,dev, device = device)\n",
    "    acc = evaluator.evaluate(batch_size = batch_size)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b79021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84434c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
