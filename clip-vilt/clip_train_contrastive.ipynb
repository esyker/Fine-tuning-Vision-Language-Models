{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a6a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "from transformers import CLIPTokenizer\n",
    "from transformers import CLIPFeatureExtractor, AutoFeatureExtractor\n",
    "from transformers import CLIPProcessor\n",
    "from transformers import CLIPModel, CLIPConfig\n",
    "from transformers import get_scheduler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546a6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "class SupervisedContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, feature_vectors, labels):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(feature_vectors, p=2, dim=1)\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2694f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset_path,image_path, vsr = None, vsr_image_path = './data/vsr-images', max_length = 77):\n",
    "        self.max_length = max_length\n",
    "        self.image_path = image_path\n",
    "        if(vsr):#assess only for the vsr dataset\n",
    "            self.dataset = self.read_vsr_dataset(vsr)\n",
    "        else:#assess for the SNLI-VE dataset\n",
    "            self.dataset = self.read_dataset(dataset_path)\n",
    "        #self.features_extract = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.features_extract = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        #self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def read_vsr_dataset(self,dataset_name, dataset_path = '../visual-spatial-reasoning/',splits_path='splits/', \n",
    "                         image_path = 'images/',sort = True, encode_labels = False):\n",
    "        dataset = pd.read_json(dataset_path+splits_path+dataset_name, lines =True)\n",
    "        dataset = dataset[['caption','image','label','relation']]\n",
    "        dataset.rename(columns = {'caption':'hypothesis', 'image':'Flickr30kID', 'label' : 'gold_label'}, inplace = True)\n",
    "        dataset['Flickr30kID']=dataset['Flickr30kID'].apply(lambda img_name: dataset_path + image_path + img_name )\n",
    "        if encode_labels:\n",
    "            labels_encoding = {0:0,1:2}#leave the label 0 the same and convert 1 to 2 to mean entailment\n",
    "            dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        if(dataset_name=='train.json'):\n",
    "            dataset.drop(labels=[1786,3569,4553,4912], axis=0, inplace = True)\n",
    "        elif(dataset_name=='test.json'):\n",
    "            dataset.drop(labels=[135,614,1071,1621,1850], axis=0, inplace = True)\n",
    "        elif(dataset_name=='dev.json'):\n",
    "            dataset.drop(labels=[807], axis=0, inplace = True)\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        if sort:\n",
    "            #dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "            dataset.sort_values(['relation'],ascending=False, inplace=True)\n",
    "        return dataset\n",
    "    \n",
    "    def read_dataset(self, url,sort = False):\n",
    "        dataset = pd.read_csv(url)\n",
    "        labels_encoding = {'contradiction':0,'neutral': 1,\n",
    "                           'entailment':2}\n",
    "        dataset = dataset[['hypothesis','Flickr30kID','gold_label']]\n",
    "        dataset['gold_label']=dataset['gold_label'].apply(lambda label: labels_encoding[label])\n",
    "        dataset['Flickr30kID']=dataset['Flickr30kID'].apply(lambda img_name: self.image_path + img_name )\n",
    "        if sort:\n",
    "            dataset.sort_values(by=\"hypothesis\", key=lambda x: x.str.len(), inplace = True)\n",
    "        return dataset\n",
    "    \n",
    "    def get_visual_features(self,img):\n",
    "        return self.features_extract(img, return_tensors=\"pt\")\n",
    "    \n",
    "    def get_text_features(self,text): \n",
    "        return self.tokenizer(text, return_tensors=\"pt\", padding = True, truncation = True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset.loc[idx]\n",
    "        img_name = sample['Flickr30kID']\n",
    "        text = sample['hypothesis']\n",
    "        label = sample['gold_label']\n",
    "        \n",
    "        item = {'text': text,\n",
    "                'image': Image.open(img_name),\n",
    "                'label': torch.tensor(label,dtype = torch.long)}\n",
    "        return item\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        #print(batch)\n",
    "        text = []\n",
    "        images = []\n",
    "        labels = []\n",
    "        for item in batch:            \n",
    "            text.append(item['text'])\n",
    "            images.append(item['image'])\n",
    "            labels.append(item['label'])\n",
    "        item_img = self.get_visual_features(images)\n",
    "        item_text = self.get_text_features(text)\n",
    "        #item = self.processor(text=text, images= images, return_tensors=\"pt\",truncation=True,padding =True)\n",
    "        item = {**item_img,**item_text}\n",
    "        item['label'] = torch.tensor(labels,dtype = torch.long)\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba5329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTrainer():\n",
    "    def __init__(self,model,train,eval_test, device = None, num_labels = 3):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train = train\n",
    "        self.eval_test = eval_test\n",
    "        self.test_acc_list = []#init\n",
    "        self.model_path = \"./models/new_my_model_epoch_\"\n",
    "        self.num_labels = num_labels\n",
    "        self.config_problem_type = \"single_label_classification\"\n",
    "        if self.config_problem_type == \"single_label_classification\":\n",
    "          self.loss_fct = SupervisedContrastiveLoss()\n",
    "          self.output_loss = lambda outputs,labels : self.loss_fct(outputs,labels) \n",
    "        self.train_loader = DataLoader(self.train, batch_size=batch_size, shuffle=False, num_workers = 4,\n",
    "                                 collate_fn = self.train.collate_fn)\n",
    "        \n",
    "    def train_model(self,batch_size = None, lr= None, epochs=None):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps= epochs * len(self.train_loader)\n",
    "        )\n",
    "        for epoch in range(epochs):\n",
    "            progress_bar = tqdm(range(math.ceil(len(self.train)/batch_size)))\n",
    "            train_losses = []\n",
    "            for item in self.train_loader:\n",
    "                \"\"\"\n",
    "                print(item.keys())\n",
    "                for key, value in item.items() :\n",
    "                    print(value.shape)\n",
    "                    print(key,'\\n',value)\n",
    "                \"\"\"\n",
    "                item['input_ids'] = item['input_ids'].to(self.device)\n",
    "                item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "                item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "                item['label'] = item['label'].to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model.forward(**item)\n",
    "                label = item['label']\n",
    "                loss = self.output_loss(outputs, label)\n",
    "                train_losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                progress_bar.update(1)\n",
    "            #print(\"Saving model ....\")\n",
    "            #model.save_model(self.model_path+str(epoch))\n",
    "            #print(\"Model Saved!\")\n",
    "            test_acc = self.eval_test.evaluate(batch_size = batch_size)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            print('--- Epoch ',epoch,' Acc: ',test_acc)\n",
    "            mean_loss = torch.tensor(train_losses).mean().item()\n",
    "            print('Training loss: %.4f' % (mean_loss))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc345869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEvaluator():\n",
    "  def __init__(self,model,test, device = None):\n",
    "    self.test_dataset = test\n",
    "    self.model = model\n",
    "    self.device = device\n",
    "  \n",
    "  def evaluate(self, batch_size = 8):\n",
    "      self.model.eval()\n",
    "      loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle = False, num_workers = 4,\n",
    "                         collate_fn = self.test_dataset.collate_fn)\n",
    "      n_correct = 0\n",
    "      n_possible = 0\n",
    "      for item in loader:\n",
    "        item['input_ids'] = item['input_ids'].to(self.device)\n",
    "        item['attention_mask'] = item['attention_mask'].to(self.device)\n",
    "        item['pixel_values'] = item['pixel_values'].to(self.device)\n",
    "        item['label'] = item['label'].to(self.device)\n",
    "        y_hat = self.model.predict(item)\n",
    "        y = item['label']\n",
    "        n_correct += (y == y_hat).sum().item()\n",
    "        n_possible += float(y.shape[0])\n",
    "      self.model.train()\n",
    "      return n_correct / n_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080ccc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(torch.nn.Module):\n",
    "    def __init__(self, clip, num_labels=3):\n",
    "      super(CLIPClassifier, self).__init__()\n",
    "      self.clip = clip\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      \"\"\"\"\n",
    "      self.classification = torch.nn.Sequential( torch.nn.Linear(in_features=512, out_features=1536, bias=True)\n",
    "        , torch.nn.GELU()\n",
    "        , torch.nn.LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
    "        , torch.nn.Linear(in_features=1536, out_features=num_labels, bias=True)\n",
    "      )\n",
    "      \"\"\"\n",
    "      \"\"\"\"\n",
    "      self.classification = torch.nn.Sequential(torch.nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "          ,torch.nn.GELU()\n",
    "          ,torch.nn.LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
    "           ,torch.nn.Linear(in_features=512, out_features=30522, bias=False)\n",
    "            , torch.nn.Linear(in_features=30522, out_features=num_labels, bias=True))\n",
    "      \"\"\"\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = self.clip.forward(input_ids,  pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict=True)\n",
    "        #print('ov',output.vision_model_output[0].size())\n",
    "        #print('ot',output.text_model_output[0].size())\n",
    "        #print('am',attention_mask.size())\n",
    "        aux_vision = output.vision_model_output[0]#.pooler_output#\n",
    "        aux_vision = self.clip.visual_projection(aux_vision)\n",
    "        aux_text = output.text_model_output[0]#.pooler_output#[0]\n",
    "        aux_text = self.clip.text_projection(aux_text)\n",
    "        aux = torch.cat((aux_vision,aux_text),dim=1)\n",
    "\n",
    "        ones = torch.ones(aux_vision.shape[0],aux_vision.shape[1],dtype=torch.float).to(device)\n",
    "        aux_mask = torch.cat((ones,attention_mask), dim=1)\n",
    "        padding_mask = torch.swapaxes(aux_mask, 0, 1)\n",
    "\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask',aux_mask.size())\n",
    "\n",
    "        aux = self.new_transformer_encoder( aux, src_key_padding_mask= padding_mask)\n",
    "        #aux = self.new_transformer_encoder( aux, mask= padding_mask)\n",
    "        #print(aux.shape)#change back shape to (batch size, sequence length, features)\n",
    "        \n",
    "        input_mask_expanded = aux_mask.unsqueeze(-1).expand(aux.size()).float()\n",
    "        #print('input_mask_expanded ',input_mask_expanded.size())\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask ',aux_mask.size())\n",
    "        #aux = torch.sum(aux * aux_mask, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        aux = torch.sum(aux * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        output.logits = self.classification(aux)\n",
    "        return output\n",
    "    \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82aff68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(CLIPModel):\n",
    "    def __init__(self, num_labels=3):\n",
    "      super().__init__(CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\"))\n",
    "      self.new_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "      self.new_transformer_encoder = torch.nn.TransformerEncoder(self.new_encoder_layer, num_layers=3)\n",
    "      self.classification = torch.nn.Linear(512, num_labels, bias=True)\n",
    "      self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, position_ids=None, return_loss=None, output_attentions=None, output_hidden_states=None, label=None):\n",
    "        output = super().forward(input_ids,  pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict=True)\n",
    "        #print('ov',output.vision_model_output[0].size())\n",
    "        #print('ot',output.text_model_output[0].size())\n",
    "        #print('am',attention_mask.size())\n",
    "        aux_vision = output.vision_model_output[0]#.pooler_output#\n",
    "        aux_vision = self.visual_projection(aux_vision)\n",
    "        aux_text = output.text_model_output[0]#.pooler_output#[0]\n",
    "        aux_text = self.text_projection(aux_text)\n",
    "        aux = torch.cat((aux_vision,aux_text),dim=1)\n",
    "\n",
    "        ones = torch.ones(aux_vision.shape[0],aux_vision.shape[1],dtype=torch.float).to(device)\n",
    "        aux_mask = torch.cat((ones,attention_mask), dim=1)\n",
    "        padding_mask = torch.swapaxes(aux_mask, 0, 1)\n",
    "\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask',aux_mask.size())\n",
    "\n",
    "        aux = self.new_transformer_encoder( aux, src_key_padding_mask= padding_mask)\n",
    "        #aux = self.new_transformer_encoder( aux, mask= padding_mask)\n",
    "        #print(aux.shape)#change back shape to (batch size, sequence length, features)\n",
    "        \n",
    "        input_mask_expanded = aux_mask.unsqueeze(-1).expand(aux.size()).float()\n",
    "        #print('input_mask_expanded ',input_mask_expanded.size())\n",
    "        #print('aux',aux.size())\n",
    "        #print('aux_mask ',aux_mask.size())\n",
    "        #aux = torch.sum(aux * aux_mask, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        aux = torch.sum(aux * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        output.logits = self.classification(aux)\n",
    "        return output\n",
    "    \n",
    "    def predict(self,item):\n",
    "      \"\"\"\n",
    "      item (n_examples x n_features)\n",
    "      \"\"\"\n",
    "      scores = model(**item)  # (n_examples x n_classes)\n",
    "      predicted_labels = scores.logits.argmax(dim=-1)  # (n_examples)\n",
    "      return predicted_labels\n",
    "  \n",
    "    def save_model(self,path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load_model(self,path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee62d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfcd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'vsr'\n",
    "task = 'train'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 1e-5\n",
    "if dataset =='vsr':\n",
    "    num_labels = 2\n",
    "elif dataset =='snli-ve':\n",
    "    num_labels =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9effaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIP(num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0075b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPClassifier(CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "                                      , num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#modules = [model.text_model.embeddings, model.vision_model.embeddings]\n",
    "modules = [model.text_model, model.vision_model]\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "train = ContrastiveDataset(\"../e-ViL/data/esnlive_train.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length, vsr= 'train.json')\n",
    "test = ContrastiveDataset(\"../e-ViL/data/esnlive_test.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length,\n",
    "                        vsr= 'test.json')\n",
    "dev = ContrastiveDataset(\"../e-ViL/data/esnlive_dev.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length, vsr= 'dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7603b8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if task =='train':\n",
    "    test_evaluator = MyEvaluator(model,test, device = device)\n",
    "    dev_evaluator = MyEvaluator(model,dev, device = device)\n",
    "    trainer = ContrastiveTrainer(model,train,test_evaluator, device = device, num_labels = num_labels)\n",
    "    print(\"-----Training Model-----\")\n",
    "    trainer.train_model(epochs=epochs ,batch_size = batch_size, lr = lr)\n",
    "    print('----Training finished-----')\n",
    "    dev_acc = dev_evaluator.evaluate(batch_size = batch_size)\n",
    "    print(\"---- Dev Acc: \",dev_acc)\n",
    "    train_acc = MyEvaluator(model,train,device=device).evaluate(batch_size = batch_size)\n",
    "    print(\"--- Train Acc: \", train_acc)\n",
    "    model.save_model(dataset+'_len'+str(max_length)+'_batch'+str(batch_size)+'_lr'+str(lr))\n",
    "elif task =='test':\n",
    "    model.load_model(\"my_model_epoch_9\")\n",
    "    evaluator = MyEvaluator(model,dev, device = device)\n",
    "    acc = evaluator.evaluate(batch_size = batch_size)\n",
    "    print(acc)\n",
    "    #output = run_example(model,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "train = MyDataset(\"../e-ViL/data/esnlive_train.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length, vsr= 'train.json')\n",
    "test = MyDataset(\"../e-ViL/data/esnlive_test.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length,\n",
    "                        vsr= 'test.json')\n",
    "dev = MyDataset(\"../e-ViL/data/esnlive_dev.csv\",\n",
    "                      \"./data/my_image_db\",\n",
    "                      max_length = max_length, vsr= 'dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc643e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c54dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task =='train':\n",
    "    test_evaluator = MyEvaluator(model,test, device = device)\n",
    "    dev_evaluator = MyEvaluator(model,dev, device = device)\n",
    "    trainer = MyTrainer(model,train,test_evaluator, device = device, num_labels = num_labels)\n",
    "    print(\"-----Training Model-----\")\n",
    "    trainer.train_model(epochs=epochs ,batch_size = batch_size, lr = lr)\n",
    "    print('----Training finished-----')\n",
    "    dev_acc = dev_evaluator.evaluate(batch_size = batch_size)\n",
    "    print(\"---- Dev Acc: \",dev_acc)\n",
    "    train_acc = MyEvaluator(model,train,device=device).evaluate(batch_size = batch_size)\n",
    "    print(\"--- Train Acc: \", train_acc)\n",
    "    model.save_model(dataset+'_len'+str(max_length)+'_batch'+str(batch_size)+'_lr'+str(lr))\n",
    "elif task =='test':\n",
    "    model.load_model(\"my_model_epoch_9\")\n",
    "    evaluator = MyEvaluator(model,dev, device = device)\n",
    "    acc = evaluator.evaluate(batch_size = batch_size)\n",
    "    print(acc)\n",
    "    #output = run_example(model,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b79021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84434c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
